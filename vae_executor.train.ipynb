{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating valid_files\n",
      "done valid_files\n",
      "dataset done\n",
      "dataloader done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/home/luke/VIU/09MIAR/src\")\n",
    "\n",
    "from vae.datasets.audio_dataset import AudioDataset\n",
    "from vae.datasets.mp3_validator import MP3ValidatorDataset\n",
    "from vae.datasources.fma_datasource import FMADatasource\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('./VIU/09MIAR/src/vae/.env')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "datasets_path = '/home/luke/VIU/09MIAR/datasets'\n",
    "valid_files_csv_path = '/home/luke/valid_files.csv'\n",
    "\n",
    "def get_dataloader(datasets_path, valid_files_csv_path, num_mels):\n",
    "    fma_dataset = FMADatasource(datasets_path)\n",
    "\n",
    "    file_paths = fma_dataset.get_file_paths()\n",
    "    labels = fma_dataset.get_labels()\n",
    "    mp3Validator = MP3ValidatorDataset(file_paths,labels,valid_files_csv_path,num_mels,10,25,int(os.environ.get('SAMPLE_RATE')))\n",
    "     \n",
    "    num_tracks_per_genre, dict_dataset = fma_dataset.balanced(mp3Validator.getValidFiles() ,int(os.environ.get('LIMIT_FILES'))) # TODO: REMOVE -> limited to 500\n",
    "\n",
    "    file_paths = list(dict_dataset.keys())\n",
    "    labels = [dict_dataset[fp]['label'] for fp in file_paths]\n",
    "\n",
    "    dataset = AudioDataset(file_paths, labels)\n",
    "    print('dataset done')\n",
    "    \n",
    "    # dataloader = DataLoader(dataset, int(os.environ.get('TRAIN_BATCH_SIZE')), shuffle=False, drop_last=True, pin_memory=False)\n",
    "    dataloader = DataLoader(dataset, int(os.environ.get('TRAIN_BATCH_SIZE')), shuffle=True, num_workers=30)\n",
    "    print('dataloader done')\n",
    "\n",
    "    return dataloader, dataset\n",
    "\n",
    "dataloader, dataset = get_dataloader(datasets_path, valid_files_csv_path, int(os.environ.get('NUM_MELS')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class VAEDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 dataset_cls=None,\n",
    "                 dataset_kwargs=None,\n",
    "                 train_dataset=None,\n",
    "                 val_dataset=None,\n",
    "                 val_split=0.2,\n",
    "                 batch_size=1,\n",
    "                 num_workers=0):\n",
    "        super().__init__()\n",
    "        self.dataset_cls = dataset_cls\n",
    "        self.dataset_kwargs = dataset_kwargs or {}\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.val_split = val_split\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        if self.train_dataset is not None and self.val_dataset is not None:\n",
    "            return\n",
    "\n",
    "\n",
    "        if self.train_dataset is not None and self.val_dataset is None:\n",
    "            total_len = len(self.train_dataset)\n",
    "            train_len = int((1 - self.val_split) * total_len)\n",
    "            val_len = total_len - train_len\n",
    "            self.train_dataset, self.val_dataset = random_split(self.train_dataset, [train_len, val_len])\n",
    "            return\n",
    "\n",
    "\n",
    "        if self.dataset_cls is None:\n",
    "            raise ValueError(\"Debe proporcionar 'dataset_cls' si no pasa ningún dataset ya instanciado.\")\n",
    "\n",
    "        full_dataset = self.dataset_cls(**self.dataset_kwargs)\n",
    "        train_size = int((1 - self.val_split) * len(full_dataset))\n",
    "        val_size = len(full_dataset) - train_size\n",
    "        self.train_dataset, self.val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPEC_TIME_STEPS 501\n",
      "GENRE_EMBEDDING_DIM 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "load_dotenv('./VIU/09MIAR/src/vae/.env')\n",
    "SAMPLE_RATE = int(os.environ[\"SAMPLE_RATE\"])\n",
    "N_FFT = int(os.environ[\"N_FFT\"])\n",
    "HOP_LENGTH = int(os.environ[\"HOP_LENGTH\"])\n",
    "NUM_MELS = int(os.environ[\"NUM_MELS\"])\n",
    "SEGMENT_DURATION = int(os.environ[\"SEGMENT_DURATION\"])\n",
    "LATENT_DIM = int(os.environ[\"LATENT_DIM\"])\n",
    "NUM_GENRES = int(os.environ[\"NUM_GENRES\"])\n",
    "\n",
    "\n",
    "SPEC_TIME_STEPS = int((SAMPLE_RATE * SEGMENT_DURATION) / HOP_LENGTH)\n",
    "print(f\"SPEC_TIME_STEPS {SPEC_TIME_STEPS}\")\n",
    "\n",
    "\n",
    "def next_power_of_two(n):\n",
    "    return 2 ** math.ceil(math.log2(n))\n",
    "\n",
    "GENRE_EMBEDDING_DIM = next_power_of_two(NUM_GENRES)\n",
    "print(f\"GENRE_EMBEDDING_DIM {GENRE_EMBEDDING_DIM}\")\n",
    "\n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.genre_embedding = nn.Embedding(NUM_GENRES, GENRE_EMBEDDING_DIM)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1 + GENRE_EMBEDDING_DIM, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_spec = torch.zeros(1, 1, NUM_MELS, SPEC_TIME_STEPS)\n",
    "            dummy_genre = torch.zeros(1, dtype=torch.long)\n",
    "            dummy_emb = self.genre_embedding(dummy_genre).view(1, GENRE_EMBEDDING_DIM, 1, 1).expand(-1, -1, NUM_MELS, SPEC_TIME_STEPS)\n",
    "            dummy_input = torch.cat([dummy_spec, dummy_emb], dim=1)\n",
    "\n",
    "            dummy = self.pool(F.relu(self.bn1(self.conv1(dummy_input))))\n",
    "            dummy = self.pool(F.relu(self.bn2(self.conv2(dummy))))\n",
    "            dummy = self.pool(F.relu(self.bn3(self.conv3(dummy))))\n",
    "\n",
    "            self.flattened_shape = dummy.shape[1:]\n",
    "            self.feature_dim = dummy.numel()\n",
    "\n",
    "        self.fc_mu = nn.Linear(self.feature_dim, LATENT_DIM)\n",
    "        self.fc_logvar = nn.Linear(self.feature_dim, LATENT_DIM)\n",
    "\n",
    "    def forward(self, x, genre):\n",
    "        x = x[..., :SPEC_TIME_STEPS]\n",
    "        genre_emb = self.genre_embedding(genre).view(genre.size(0), GENRE_EMBEDDING_DIM, 1, 1)\n",
    "        genre_emb = genre_emb.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, genre_emb], dim=1)\n",
    "\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "\n",
    "        \n",
    "        logvar = torch.clamp(logvar, min=-10.0, max=10.0)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, feature_dim, flattened_shape):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.flattened_shape = flattened_shape\n",
    "\n",
    "        self.genre_embedding = nn.Embedding(NUM_GENRES, GENRE_EMBEDDING_DIM)\n",
    "        self.fc = nn.Linear(LATENT_DIM + GENRE_EMBEDDING_DIM, feature_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=1, batch_first=True)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.output_pad = nn.ConstantPad2d((0, 5), 0)\n",
    "\n",
    "    def forward(self, z, genre):\n",
    "        genre_emb = self.genre_embedding(genre)\n",
    "        z = torch.cat([z, genre_emb], dim=1)\n",
    "\n",
    "        x = self.fc(z)\n",
    "        x = x.view(z.size(0), *self.flattened_shape)\n",
    "\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        B, H, C, W = x.shape\n",
    "        x = x.reshape(B * H, C, W).permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.permute(0, 2, 1).reshape(B, H, C, W)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        \n",
    "        x = self.deconv3(x)\n",
    "        x = self.output_pad(x)\n",
    "\n",
    "        \n",
    "        # if not self.training:\n",
    "        #     import matplotlib.pyplot as plt\n",
    "\n",
    "        #     flat = x.detach().cpu().flatten()\n",
    "        #     x_min = flat.min().item()\n",
    "        #     x_max = flat.max().item()\n",
    "        #     x_mean = flat.mean().item()\n",
    "        #     x_std = flat.std().item()\n",
    "        #     sat_low = (flat <= -0.95).float().mean().item()\n",
    "        #     sat_high = (flat >= 0.95).float().mean().item()\n",
    "\n",
    "        #     print(f\"[DEBUG] Decoder output stats -> min: {x_min:.4f}, max: {x_max:.4f}, mean: {x_mean:.4f}, std: {x_std:.4f}\")\n",
    "        #     print(f\"[DEBUG] Saturación -> <= -0.95: {sat_low:.2%}, >= 0.95: {sat_high:.2%}\")\n",
    "\n",
    "        #     plt.figure(figsize=(6, 4))\n",
    "        #     plt.hist(flat.numpy(), bins=50, color='purple', alpha=0.75)\n",
    "        #     plt.title(\"Histograma de valores del decoder\")\n",
    "        #     plt.xlabel(\"Valor\")\n",
    "        #     plt.ylabel(\"Frecuencia\")\n",
    "        #     plt.grid(True)\n",
    "        #     plt.tight_layout()\n",
    "        #     plt.show()\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = VAE_Encoder()\n",
    "        self.decoder = VAE_Decoder(self.encoder.feature_dim, self.encoder.flattened_shape)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self,m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.uniform_(m.weight, -0.1, 0.1)\n",
    "        elif isinstance(m, nn.LSTM):\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.zeros_(param)\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, genre):\n",
    "        # print(f\"[ENCODER IN] x mean: {x.mean().item():.4f}, std: {x.std().item():.4f}\")\n",
    "        mu, logvar = self.encoder(x, genre)\n",
    "        # print(f\"[LATENT] mu mean: {mu.mean().item():.4f}, std: {mu.std().item():.4f}\")\n",
    "        # print(f\"[LATENT] logvar mean: {logvar.mean().item():.4f}, std: {logvar.std().item():.4f}\")\n",
    "\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decoder(z, genre)\n",
    "        if not torch.isfinite(x_hat).all():\n",
    "            print(\"[ERROR] x_hat contiene NaNs\")\n",
    "\n",
    "        # print(f\"[DECODER OUT] x_recon mean: {x_hat.mean().item():.4f}, std: {x_hat.std().item():.4f}\")\n",
    "\n",
    "\n",
    "        if not torch.isfinite(x).all():\n",
    "            print(\"[ERROR] Input contiene NaNs o infinitos\")\n",
    "        if not torch.isfinite(mu).all():\n",
    "            print(\"[ERROR] mu contiene NaNs\")\n",
    "        if not torch.isfinite(logvar).all():\n",
    "            print(\"[ERROR] logvar contiene NaNs\")\n",
    "\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "\n",
    "\n",
    "BETA_MAX = float(os.environ[\"BETA_MAX\"])\n",
    "BETA_WARMUP_EPOCHS = int(os.environ[\"BETA_WARMUP_EPOCHS\"])\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "class LitVAE(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = VAE()\n",
    "        self.register_buffer(\"x_sum\", torch.tensor(0.0))\n",
    "        self.register_buffer(\"x_squared_sum\", torch.tensor(0.0))\n",
    "        self.register_buffer(\"x_count\", torch.tensor(0))\n",
    "        self.std_x = 0\n",
    "        self.mean_x = 0\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "\n",
    "        self.genre_db_limits_path = \"logs/genre_db_limits.json\"\n",
    "        self.genre_db_limits = self.load_or_init_genre_limits()\n",
    "\n",
    "        self.train_step_metrics = []\n",
    "        self.val_step_metrics = []\n",
    "        self.train_epoch_metrics = []\n",
    "        self.val_epoch_metrics = []\n",
    "\n",
    "        self.register_buffer(\"mean_x_buffer\", torch.tensor(0.0))\n",
    "        self.register_buffer(\"std_x_buffer\", torch.tensor(1.0))\n",
    "\n",
    "        os.makedirs(\"logs/csv\", exist_ok=True)\n",
    "        os.makedirs(\"logs/img\", exist_ok=True)\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    def load_or_init_genre_limits(self):\n",
    "        import json\n",
    "        if os.path.exists(self.genre_db_limits_path):\n",
    "            with open(self.genre_db_limits_path, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        return {str(i): [100.0, -100.0] for i in range(NUM_GENRES)}\n",
    "\n",
    "    def update_genre_limits(self, genre_id, spec):\n",
    "        import numpy as np\n",
    "        spec_db = 20 * torch.log10(torch.clamp(spec, min=1e-5)).cpu().numpy()\n",
    "        min_db = float(np.min(spec_db))\n",
    "        max_db = float(np.max(spec_db))\n",
    "        g = str(genre_id)\n",
    "        if g not in self.genre_db_limits:\n",
    "            self.genre_db_limits[g] = [min_db, max_db]\n",
    "        else:\n",
    "            current_min, current_max = self.genre_db_limits[g]\n",
    "            self.genre_db_limits[g][0] = min(current_min, min_db)\n",
    "            self.genre_db_limits[g][1] = max(current_max, max_db)\n",
    "\n",
    "    def save_genre_limits(self):\n",
    "        import json\n",
    "        with open(self.genre_db_limits_path, \"w\") as f:\n",
    "            json.dump(self.genre_db_limits, f, indent=2)\n",
    "\n",
    "    def forward(self, x, genre):\n",
    "        x = x[..., :SPEC_TIME_STEPS]\n",
    "        x_hat, mu, logvar = self.model(x, genre)\n",
    "        x_hat = x_hat[..., :SPEC_TIME_STEPS]\n",
    "        if x_hat.shape[-1] != x.shape[-1]:\n",
    "            min_width = min(x_hat.shape[-1], x.shape[-1])\n",
    "            x = x[..., :min_width]\n",
    "            x_hat = x_hat[..., :min_width]\n",
    "        return x_hat, x, mu, logvar\n",
    "\n",
    "    def compute_loss(self, x_hat, x, mu, logvar):\n",
    "        warmup = max(BETA_WARMUP_EPOCHS, 1)\n",
    "        step = self.current_epoch / warmup\n",
    "        beta = float(BETA_MAX / (1 + math.exp(-10 * (step - 0.5))))\n",
    "        beta = min(beta, BETA_MAX)\n",
    "\n",
    "        recon_loss = F.mse_loss(x_hat, x, reduction='mean')\n",
    "        kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "        loss = recon_loss + beta * kl_div\n",
    "        return loss, recon_loss, kl_div, beta\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, genre = batch\n",
    "        x_hat, x, mu, logvar = self(x, genre)\n",
    "\n",
    "        self.x_sum += x.sum()\n",
    "        self.x_squared_sum += (x ** 2).sum()\n",
    "        self.x_count += x.numel()\n",
    "\n",
    "        for i in range(x.size(0)):\n",
    "            self.update_genre_limits(genre[i].item(), x[i])\n",
    "\n",
    "        loss, recon_loss, kl_div, beta = self.compute_loss(x_hat, x, mu, logvar)\n",
    "\n",
    "        self.log(\"loss\", loss, prog_bar=True)\n",
    "        self.log(\"recon_loss\", recon_loss, prog_bar=True)\n",
    "        self.log(\"kl_div\", kl_div, prog_bar=True)\n",
    "        self.log(\"beta\", beta, prog_bar=True)\n",
    "\n",
    "        self.train_step_metrics.append({\n",
    "            \"epoch\": self.current_epoch,\n",
    "            \"batch\": batch_idx,\n",
    "            \"loss\": loss.item(),\n",
    "            \"recon_loss\": recon_loss.item(),\n",
    "            \"kl_div\": kl_div.item(),\n",
    "            \"beta\": beta\n",
    "        })\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, genre = batch\n",
    "        x_hat, x, mu, logvar = self(x, genre)\n",
    "        loss, recon_loss, kl_div, beta = self.compute_loss(x_hat, x, mu, logvar)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_recon_loss\", recon_loss, prog_bar=True)\n",
    "        self.log(\"val_kl_div\", kl_div, prog_bar=True)\n",
    "\n",
    "        self.val_step_metrics.append({\n",
    "            \"epoch\": self.current_epoch,\n",
    "            \"batch\": batch_idx,\n",
    "            \"val_loss\": loss.item(),\n",
    "            \"val_recon_loss\": recon_loss.item(),\n",
    "            \"val_kl_div\": kl_div.item(),\n",
    "            \"beta\": beta\n",
    "        })\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.val_step_metrics:\n",
    "            val_losses = [m[\"val_loss\"] for m in self.val_step_metrics if \"val_loss\" in m]\n",
    "            mean_val_loss = float(np.mean(val_losses))\n",
    "            self.val_epoch_metrics.append({\n",
    "                \"epoch\": self.current_epoch,\n",
    "                \"val_loss\": mean_val_loss\n",
    "            })\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        import torch\n",
    "        if self.x_count > 0:\n",
    "            mean_x = self.x_sum / self.x_count\n",
    "            std_x = torch.sqrt(self.x_squared_sum / self.x_count - mean_x ** 2)\n",
    "            self.mean_x = mean_x.item()\n",
    "            self.std_x = std_x.item()\n",
    "            self.log(\"x_mean_epoch\", mean_x, prog_bar=True)\n",
    "            self.log(\"x_std_epoch\", std_x, prog_bar=True)\n",
    "            print(f\"[INFO] Época {self.current_epoch} completada — mean_x={self.mean_x:.4f}, std_x={self.std_x:.4f}\")\n",
    "            self.mean_x_buffer.copy_(mean_x)\n",
    "            self.std_x_buffer.copy_(std_x)\n",
    "\n",
    "\n",
    "        self.x_sum.zero_()\n",
    "        self.x_squared_sum.zero_()\n",
    "        self.x_count.zero_()\n",
    "\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        import torch\n",
    "\n",
    "        if self.train_step_metrics:\n",
    "            df_train = pd.DataFrame(self.train_step_metrics)\n",
    "            df_train.to_csv(\"logs/csv/metrics_train_step.csv\", mode='a', header=not os.path.exists(\"logs/csv/metrics_train_step.csv\"), index=False)\n",
    "            self.train_step_metrics.clear()\n",
    "\n",
    "        if self.val_step_metrics:\n",
    "            df_val = pd.DataFrame(self.val_step_metrics)\n",
    "            df_val.to_csv(\"logs/csv/metrics_val_step.csv\", mode='a', header=not os.path.exists(\"logs/csv/metrics_val_step.csv\"), index=False)\n",
    "            self.val_step_metrics.clear()\n",
    "\n",
    "        epoch_metrics = {\n",
    "            \"epoch\": self.current_epoch,\n",
    "            \"x_mean\": self.mean_x,\n",
    "            \"x_std\": self.std_x\n",
    "        }\n",
    "        self.train_epoch_metrics.append(epoch_metrics)\n",
    "        df_epoch = pd.DataFrame(self.train_epoch_metrics)\n",
    "        df_epoch.to_csv(\"logs/csv/metrics_train_epoch.csv\", index=False)\n",
    "\n",
    "        if self.val_epoch_metrics:\n",
    "            df_val_epoch = pd.DataFrame(self.val_epoch_metrics)\n",
    "            df_val_epoch.to_csv(\"logs/csv/metrics_val_epoch.csv\", index=False)\n",
    "\n",
    "        extra = {\n",
    "            \"mean_x\": self.mean_x,\n",
    "            \"std_x\": self.std_x\n",
    "        }\n",
    "\n",
    "        if len(df_epoch) > 1:\n",
    "            plt.figure()\n",
    "            plt.plot(df_epoch[\"epoch\"], df_epoch[\"x_mean\"], label=\"x_mean\")\n",
    "            plt.plot(df_epoch[\"epoch\"], df_epoch[\"x_std\"], label=\"x_std\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Valor\")\n",
    "            plt.legend()\n",
    "            plt.title(\"Media y desviación estándar de x\")\n",
    "            plt.savefig(f\"logs/img/stats_epoch_{self.current_epoch}.jpg\")\n",
    "            plt.close()\n",
    "\n",
    "        torch.save({\"model\": self.state_dict(), \"extra\": extra}, f'''checkpoints/vae_last_{self.current_epoch}.pt''')\n",
    "\n",
    "        if self.val_epoch_metrics:\n",
    "            current_val_loss = self.val_epoch_metrics[-1][\"val_loss\"]\n",
    "            if current_val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = current_val_loss\n",
    "                torch.save({\"model\": self.state_dict(), \"extra\": extra}, \"checkpoints/vae_best.pt\")\n",
    "                torch.save(self.state_dict(), f\"checkpoints/vae_best.pt\")\n",
    "                print(f\"[INFO] Nuevo mejor modelo guardado (val_loss={current_val_loss:.4f})\")\n",
    "\n",
    "        self.save_genre_limits()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=5, factor=0.5, verbose=True)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LitVAE()\n",
    "\n",
    "# \n",
    "# trainer = pl.Trainer(max_epochs=int(os.environ.get('TRAIN_EPOCHS')))\n",
    "\n",
    "# \n",
    "# trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2025-03-31 09:41:51.472563: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-31 09:41:51.490303: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-31 09:41:51.942876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/luke/jupyter_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | VAE  | 391 M  | train\n",
      "---------------------------------------\n",
      "391 M     Trainable params\n",
      "0         Non-trainable params\n",
      "391 M     Total params\n",
      "1,566.429 Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0a17afcbbb4d1684b890367b10c4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4675164aae0944868674918b10fccfe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c9a3aa89684a469257965d923fd071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Época 0 completada — mean_x=-0.0000, std_x=0.9995\n",
      "[INFO] Nuevo mejor modelo guardado (val_loss=3965.3862)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = LitVAE()\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "data_module = VAEDataModule(\n",
    "    train_dataset=dataset,\n",
    "    val_split=0.2,\n",
    "    batch_size=int(os.environ.get('TRAIN_BATCH_SIZE')),\n",
    "    num_workers=32\n",
    ")\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=int(os.environ.get('TRAIN_EPOCHS')),\n",
    "    accelerator=\"auto\",\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "trainer.fit(model, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_audio_from_noise(model: LitVAE, genre_id: int, output_path: str = \"generated.wav\"):\n",
    "    import json\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    vae = model.model\n",
    "\n",
    "    with open(model.genre_db_limits_path, \"r\") as f:\n",
    "        genre_db_limits = json.load(f)\n",
    "    db_min, db_max = genre_db_limits[str(genre_id)]\n",
    "\n",
    "    z = torch.randn(1, LATENT_DIM).to(device)\n",
    "    genre = torch.tensor([genre_id], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        spec = vae.decoder(z, genre)\n",
    "\n",
    "    spec = spec.squeeze().cpu()\n",
    "\n",
    "    \n",
    "    spec_min = spec.min().item()\n",
    "    spec_max = spec.max().item()\n",
    "    print(f\"[DEBUG] spec antes de denormalizar: min={spec_min:.4f}, max={spec_max:.4f}\")\n",
    "\n",
    "    if abs(spec_max - spec_min) < 1e-3:\n",
    "        print(\"[WARNING] El espectrograma generado tiene un rango demasiado pequeño\")\n",
    "\n",
    "    \n",
    "    try:\n",
    "        mean_x = model.mean_x_buffer.item() if hasattr(model.mean_x_buffer, 'item') else float(model.mean_x_buffer)\n",
    "        std_x = model.std_x_buffer.item() if hasattr(model.std_x_buffer, 'item') else float(model.std_x_buffer)\n",
    "        print(f\"[INFO] Usando mean_x={mean_x}, std_x={std_x} para denormalización temporal\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Acceso a mean_x o std_x: {e}\")\n",
    "        mean_x, std_x = 0.0, 1.0\n",
    "\n",
    "    if std_x == 0:\n",
    "        print(\"[WARNING] std_x es 0, se omite denormalización\")\n",
    "    else:\n",
    "        spec = spec * std_x + mean_x\n",
    "\n",
    "    # spec = torch.clamp(spec,min=-0.25,max=0.25)\n",
    "\n",
    "    spec_db = spec * (db_max - db_min) + db_min\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(spec_db.numpy(), aspect='auto', origin='lower', cmap='magma')\n",
    "    plt.title(f\"Espectrograma generado (género {genre_id})\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    inverse_mel = torchaudio.transforms.InverseMelScale(\n",
    "        n_stft=N_FFT // 2 + 1,\n",
    "        n_mels=NUM_MELS,\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "    )\n",
    "    griffin_lim = torchaudio.transforms.GriffinLim(\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    spec_amp = torch.pow(10.0, spec_db / 20.0)\n",
    "    spec_amp = spec_amp.unsqueeze(0)\n",
    "    linear_spec = inverse_mel(spec_amp)\n",
    "    waveform = griffin_lim(linear_spec)\n",
    "\n",
    "    torchaudio.save(output_path, waveform, SAMPLE_RATE)\n",
    "    print(f\"Audio generado guardado en: {output_path}\")\n",
    "\n",
    "    return waveform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitVAE()\n",
    "model.load_state_dict(torch.load(\"checkpoints/vae_best.pt\", map_location=\"cuda\"))\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "generate_audio_from_noise(model, genre_id=0, output_path=\"sample_genre0.wav\")\n",
    "generate_audio_from_noise(model, genre_id=1, output_path=\"sample_genre1.wav\")\n",
    "generate_audio_from_noise(model, genre_id=2, output_path=\"sample_genre2.wav\")\n",
    "generate_audio_from_noise(model, genre_id=3, output_path=\"sample_genre3.wav\")\n",
    "generate_audio_from_noise(model, genre_id=4, output_path=\"sample_genre4.wav\")\n",
    "generate_audio_from_noise(model, genre_id=7, output_path=\"sample_genre7.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "\n",
    "\n",
    "x, y = batch\n",
    "\n",
    "print(\"Shape del batch completo (x):\", x.shape)\n",
    "print(\"Shape del target (y):\", y.shape if y is not None else \"None\")\n",
    "\n",
    "\n",
    "print(\"\\nEjemplo individual:\")\n",
    "ejemplo = x[0]\n",
    "print(\"Shape del ejemplo:\", ejemplo.shape)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if ejemplo.ndim == 3:\n",
    "\n",
    "    plt.imshow(ejemplo.squeeze().cpu(), aspect='auto', origin='lower')\n",
    "    plt.title(\"Espectrograma del primer segmento\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "elif ejemplo.ndim == 4:\n",
    "\n",
    "    print(\"Número de segmentos:\", ejemplo.shape[0])\n",
    "    fig, axs = plt.subplots(1, ejemplo.shape[0], figsize=(15, 3))\n",
    "    for i in range(ejemplo.shape[0]):\n",
    "        axs[i].imshow(ejemplo[i].squeeze().cpu(), aspect='auto', origin='lower')\n",
    "        axs[i].set_title(f\"Segmento {i+1}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Formato no esperado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter_env)",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
